{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting all code together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\ProgramData\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: cymem.cymem.Pool size changed, may indicate binary incompatibility. Expected 48 from C header, got 64 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: cymem.cymem.Address size changed, may indicate binary incompatibility. Expected 24 from C header, got 40 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP_Task ready to use.\n"
     ]
    }
   ],
   "source": [
    "# import utility libraries\n",
    "import util\n",
    "import preprocessing\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlp_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP_Task ready to use.\n"
     ]
    }
   ],
   "source": [
    "nlp_task = nlp_util.NLP_Task()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load files\n",
    "tr_file, va_file, te_file = util.load_files()\n",
    "tr_dict = util.tsv_to_dict(tsv_file=tr_file)\n",
    "va_dict = util.tsv_to_dict(tsv_file=va_file)\n",
    "te_dict = util.tsv_to_dict(tsv_file=te_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_file['preprocessed'] = preprocessing.preprocessing_txt(dataset=tr_file)\n",
    "va_file['preprocessed'] = preprocessing.preprocessing_txt(dataset=va_file)\n",
    "te_file['preprocessed'] = preprocessing.preprocessing_txt(dataset=te_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['barely-true', 'false', 'half-true', 'mostly-true', 'pants-fire',\n",
       "       'true'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(tr_file['Label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'plotting_util' from 'D:\\\\UIC\\\\Fall 2018\\\\Statistical NLP\\\\Project\\\\jurat-aldo-project\\\\cs-521-project.git\\\\source\\\\CS-521-PROJECT\\\\plotting_util.py'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotting_util\n",
    "importlib.reload(plotting_util)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_by_label = [len(tr_file['Label'].values[tr_file['Label'] == 'false']),\n",
    "                len(tr_file['Label'].values[tr_file['Label'] == 'true']),\n",
    "                len(tr_file['Label'].values[tr_file['Label'] == 'barely-true']),\n",
    "                len(tr_file['Label'].values[tr_file['Label'] == 'half-true']),\n",
    "                len(tr_file['Label'].values[tr_file['Label'] == 'mostly-true']),\n",
    "                len(tr_file['Label'].values[tr_file['Label'] == 'pants-fire'])\n",
    "                ]\n",
    "data_labels = ['false', 'true','barely-true','half-true','mostly-true','pants-fire']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#bar_plot = plotting_util.plot_bar_chart(chartname='Liar liar dataset distribution', barnames=data_labels, barvalues=data_by_label,\n",
    "#                             barcolors=['skyblue'])\n",
    "#plt.plot()\n",
    "pie_plot = plotting_util.plot_pie_chart(chartname='myplot', labels=data_labels, values=data_by_label)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting POS tags grouped by unigrams, bigrams and trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting POS Tags\n",
      "Finished\n",
      "Extracting POS Tags\n",
      "Finished\n",
      "Extracting POS Tags\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "# POS extracted from no preprocessed data for training, validation and testing files\n",
    "unigram_pos, bigrams_pos, trigram_pos = preprocessing.extract_POS(statements=tr_dict['statement'])\n",
    "unigram_pos_va, bigrams_pos_va, trigram_pos_va = preprocessing.extract_POS(statements=va_dict['statement'])\n",
    "unigram_pos_te, bigrams_pos_te, trigram_pos_te = preprocessing.extract_POS(statements=te_dict['statement'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting POS Tags\n",
      "Finished\n",
      "Extracting POS Tags\n",
      "Finished\n",
      "Extracting POS Tags\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "# Pos extracted from preprocessed data for training, validation and testing files\n",
    "unigram_pos_p, bigrams_pos_p, trigram_pos_p = preprocessing.extract_POS(tr_file['preprocessed'].values)\n",
    "unigram_pos_p_va, bigrams_pos_p_va, trigram_pos_p_va = preprocessing.extract_POS(va_file['preprocessed'].values)\n",
    "unigram_pos_p_te, bigrams_pos_p_te, trigram_pos_p_te = preprocessing.extract_POS(te_file['preprocessed'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get unique values for unigrams, bigrams and trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique list of unigrams, bigrams and trigrams for no preprocessed data\n",
    "unigram_list_tr = nlp_task.UniquePosTags(unigram_pos)\n",
    "bigram_list_tr = nlp_task.UniquePosTags(bigrams_pos)\n",
    "trigram_list_tr = nlp_task.UniquePosTags(trigram_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique list of unigrams, bigrams and trigrams for preprocessed data\n",
    "unigram_list_tr_processed = nlp_task.UniquePosTags(unigram_pos_p)\n",
    "bigram_list_tr_processed = nlp_task.UniquePosTags(bigrams_pos_p)\n",
    "trigram_list_tr_processed = nlp_task.UniquePosTags(trigram_pos_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing duplicated POS in bigrams and trigrams [NNP and CD]\n",
    "\n",
    "For example: The/DT economy/NN bled/VBD $/$ 24/CD billion/CD due/JJ to/TO the/DT government/NN shutdown/NN ./. <br>\n",
    "In this case having CD_CD is the same as having only CD<br>\n",
    "Same with: <br>\n",
    "U.S./NNP Rep./NNP Ron/NNP Kind/NNP ,/, D-Wis./NNP ,/, and/CC his/PRP$ fellow/JJ Democrats/NNS went/VBD on/IN a/DT spending/NN spree/NN and/CC now/RB their/PRP$ credit/NN card/NN is/VBZ maxed/VBN out/RP <br>\n",
    "We don't need all those NNPs to find a pattern and it might be noisy to the ML algorithm<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For raw data\n",
    "\n",
    "list_to_remove = ['NNP','CD']\n",
    "# Training\n",
    "removed_pos = nlp_task.RemoveConsecutiveTags(list_to_remove,unigram_pos)\n",
    "removed_pos_bigrams = nlp_task.POS_groupping(grams=2,sentences_pos=removed_pos)\n",
    "removed_pos_trigrams = nlp_task.POS_groupping(grams=3,sentences_pos=removed_pos)\n",
    "\n",
    "# Validdation\n",
    "removed_pos_va =  nlp_task.RemoveConsecutiveTags(list_to_remove,unigram_pos_va)\n",
    "removed_pos_bigrams_va = nlp_task.POS_groupping(grams=2,sentences_pos=removed_pos_va)\n",
    "removed_pos_trigrams_va = nlp_task.POS_groupping(grams=3,sentences_pos=removed_pos_va)\n",
    "\n",
    "# Testing\n",
    "removed_pos_te =  nlp_task.RemoveConsecutiveTags(list_to_remove,unigram_pos_te)\n",
    "removed_pos_bigrams_te = nlp_task.POS_groupping(grams=2,sentences_pos=removed_pos_te)\n",
    "removed_pos_trigrams_te = nlp_task.POS_groupping(grams=3,sentences_pos=removed_pos_te)\n",
    "\n",
    "#LIST OF UNIQUE BIGRAMS AND TRIGRAMS AFTER REMOVING CONSECUTIVE SAME TAGS\n",
    "removed_unique_bigrams = nlp_task.UniquePosTags(postags=removed_pos_bigrams)\n",
    "removed_unique_trigrams = nlp_task.UniquePosTags(postags=removed_pos_trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For processed data\n",
    "list_to_remove = ['NNP','CD']\n",
    "# Training\n",
    "removed_pos_p = nlp_task.RemoveConsecutiveTags(list_to_remove,unigram_pos_p)\n",
    "removed_pos_bigrams_p = nlp_task.POS_groupping(grams=2,sentences_pos=removed_pos_p)\n",
    "removed_pos_trigrams_p = nlp_task.POS_groupping(grams=3,sentences_pos=removed_pos_p)\n",
    "\n",
    "# Validdation\n",
    "removed_pos_va_p =  nlp_task.RemoveConsecutiveTags(list_to_remove,unigram_pos_p_va)\n",
    "removed_pos_bigrams_va_p = nlp_task.POS_groupping(grams=2,sentences_pos=removed_pos_va_p)\n",
    "removed_pos_trigrams_va_p = nlp_task.POS_groupping(grams=3,sentences_pos=removed_pos_va_p)\n",
    "\n",
    "# Testing\n",
    "removed_pos_te_p =  nlp_task.RemoveConsecutiveTags(list_to_remove,unigram_pos_p_te)\n",
    "removed_pos_bigrams_te_p = nlp_task.POS_groupping(grams=2,sentences_pos=removed_pos_te_p)\n",
    "removed_pos_trigrams_te_p = nlp_task.POS_groupping(grams=3,sentences_pos=removed_pos_te_p)\n",
    "\n",
    "#LIST OF UNIQUE BIGRAMS AND TRIGRAMS AFTER REMOVING CONSECUTIVE SAME TAGS\n",
    "removed_unique_bigrams_p = nlp_task.UniquePosTags(postags=removed_pos_bigrams_p)\n",
    "removed_unique_trigrams_p = nlp_task.UniquePosTags(postags=removed_pos_trigrams_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add numerical labels for each of the sentence in the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels for multiclassification and binary classification tasks\n",
    "multi_labels = {'false':0, 'true':1,'pants-fire':2,'barely-true':3,'half-true':4,'mostly-true':5}\n",
    "binary_labels = {'false':1, 'true':-1,'pants-fire':1,'barely-true':1,'half-true':0,'mostly-true':-1}\n",
    "\n",
    "\n",
    "tr_file['b_label'] = np.array(preprocessing.create_labels(labels=tr_file['Label'].values,label_values=binary_labels))\n",
    "va_file['b_label'] = np.array(preprocessing.create_labels(labels=va_file['Label'].values,label_values=binary_labels))\n",
    "te_file['b_label'] = np.array(preprocessing.create_labels(labels=te_file['Label'].values,label_values=binary_labels))\n",
    "\n",
    "tr_file['m_label'] = np.array(preprocessing.create_labels(labels=tr_file['Label'].values,label_values=multi_labels))\n",
    "va_file['m_label'] = np.array(preprocessing.create_labels(labels=va_file['Label'].values,label_values=multi_labels))\n",
    "te_file['m_label'] = np.array(preprocessing.create_labels(labels=te_file['Label'].values,label_values=multi_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop unnecesary columns from dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once this code is executed and you try to re run it again it is going to show an error because columns already were remoded\n",
    "unnecesary_columns = ['BT', 'FC', 'HT', 'MT', 'PF']\n",
    "tr_file = tr_file.drop(unnecesary_columns, axis=1)\n",
    "va_file = va_file.drop(unnecesary_columns, axis=1)\n",
    "te_file = te_file.drop(unnecesary_columns, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract POS features from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP_Task ready to use.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'preprocessing' from 'D:\\\\UIC\\\\Fall 2018\\\\Statistical NLP\\\\Project\\\\jurat-aldo-project\\\\cs-521-project.git\\\\source\\\\CS-521-PROJECT\\\\preprocessing.py'>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having ran some experiments with decision trees regarding POS we came up with a list of POS unigrams, bigrams and trigrams that were more relevant for classifying mostly ture and mostly false news.\n",
    "<br>\n",
    "POS unigrams: ['VBZ', 'DT', 'NNPS', 'VBP', 'JJ', 'IN', 'WRB', 'VBD', 'PRP', 'RP', 'WDT', 'VB', 'NNP', 'VBG', 'PRP$', 'VBN', 'CD', 'RB', 'WP', 'JJS', 'JJR', 'EX', 'RBS', 'FW', 'LS']\n",
    " <br>\n",
    "POS brigrams: ['NNPS_VBP', 'VB_NNP', 'IN_DT', 'VB_JJ', 'JJ_CD', 'CD_NNS', 'DT_JJS', 'JJR_IN', 'IN_CD', 'CC_IN', 'RB_VBD', 'CD_NN', 'NN_TO', 'JJR_JJ', 'VB_CD'] <br>\n",
    "POS trigrams: ['VBD_VBN_IN', 'IN_DT_JJ', 'CD_NN_IN', 'IN_CD_NNS', 'IN_DT_NN', 'DT_JJ_CD', 'MD_VB_IN', 'JJS_JJ_NN', 'CC_JJ_NNS', 'JJ_NNS_VBP', 'VBP_CD_NN', 'NNS_,_CD', 'sos_JJR_IN', 'IN_DT_NNS','JJ_NN_MD'] <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_relevant_unigrams =  ['VBZ', 'DT', 'NNPS', 'VBP', 'JJ', 'IN', 'WRB', 'VBD', 'PRP', 'RP', 'WDT', 'VB', 'NNP', 'VBG', 'PRP$', 'VBN', 'CD', 'RB', 'WP', 'JJS', 'JJR', 'EX', 'RBS', 'FW', 'LS'] \n",
    "pos_relevant_bigrams = ['NNPS_VBP', 'VB_NNP', 'IN_DT', 'VB_JJ', 'JJ_CD', 'CD_NNS', 'DT_JJS', 'JJR_IN', 'IN_CD', 'CC_IN', 'RB_VBD', 'CD_NN', 'NN_TO', 'JJR_JJ', 'VB_CD'] \n",
    "pos_relevant_trigrams = ['VBD_VBN_IN', 'IN_DT_JJ', 'CD_NN_IN', 'IN_CD_NNS', 'IN_DT_NN', 'DT_JJ_CD', 'MD_VB_IN', 'JJS_JJ_NN', 'CC_JJ_NNS', 'JJ_NNS_VBP', 'VBP_CD_NN', 'sos_JJR_IN', 'IN_DT_NNS','JJ_NN_MD'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vbz': 0, 'dt': 1, 'nnps': 2, 'vbp': 3, 'jj': 4, 'in': 5, 'wrb': 6, 'vbd': 7, 'prp': 8, 'rp': 9, 'wdt': 10, 'vb': 11, 'nnp': 12, 'vbg': 13, 'prpdollar': 14, 'vbn': 15, 'cd': 16, 'rb': 17, 'wp': 18, 'jjs': 19, 'jjr': 20, 'ex': 21, 'rbs': 22, 'fw': 23, 'ls': 24}\n"
     ]
    }
   ],
   "source": [
    "Xtr_onehot_unigram, Xtr_count_unigram, Xtr_tfidf_unigram, Xval_onehot_unigram, Xval_count_unigram, Xval_tfidf_unigram, Xte_onehot_unigram, Xte_count_unigram, Xte_tfidf_unigram = util.GetFeaturesFromPOS(training_data=unigram_pos_p, validation_data=unigram_pos_p_va, testing_data=unigram_pos_p_te, user_defined_vocabulary=pos_relevant_unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5452, 8040, 1275, 3146, 6509, 9168,  568, 4337, 2964,  622,  918,\n",
       "       3859, 7858, 2608, 2034, 3381, 4617, 3962,  557,  648, 1046,  288,\n",
       "        142,   15,    7], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(Xtr_onehot_unigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nnps_vbp': 0, 'vb_nnp': 1, 'in_dt': 2, 'vb_jj': 3, 'jj_cd': 4, 'cd_nns': 5, 'dt_jjs': 6, 'jjr_in': 7, 'in_cd': 8, 'cc_in': 9, 'rb_vbd': 10, 'cd_nn': 11, 'nn_to': 12, 'jjr_jj': 13, 'vb_cd': 14}\n"
     ]
    }
   ],
   "source": [
    "Xtr_onehot_bigrams, Xtr_count_bigrams, Xtr_tfidf_bigrams, Xval_onehot_bigrams, Xval_count_bigrams, Xval_tfidf_bigrams, Xte_onehot_bigrams, Xte_count_bigrams, Xte_tfidf_bigrams = util.GetFeaturesFromPOS(training_data=removed_pos_bigrams_p, validation_data=removed_pos_bigrams_va_p, testing_data=removed_pos_bigrams_te_p, user_defined_vocabulary=pos_relevant_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 195,  198, 4851,  311,  376, 1324,  411,  551, 1658,  169,  378,\n",
       "       1325, 1016,   31,  223], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(Xtr_onehot_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vbd_vbn_in': 0, 'in_dt_jj': 1, 'cd_nn_in': 2, 'in_cd_nns': 3, 'in_dt_nn': 4, 'dt_jj_cd': 5, 'md_vb_in': 6, 'jjs_jj_nn': 7, 'cc_jj_nns': 8, 'jj_nns_vbp': 9, 'vbp_cd_nn': 10, 'sos_jjr_in': 11, 'in_dt_nns': 12, 'jj_nn_md': 13}\n"
     ]
    }
   ],
   "source": [
    "Xtr_onehot_trigram, Xtr_count_trigram, Xtr_tfidf_trigram, Xval_onehot_trigram, Xval_count_trigram, Xval_tfidf_trigram, Xte_onehot_trigram, Xte_count_trigram, Xte_tfidf_trigram = util.GetFeaturesFromPOS(training_data=trigram_pos, validation_data=trigram_pos_va, testing_data=trigram_pos_te, user_defined_vocabulary=pos_relevant_trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 183, 1497,  791,  458, 2454,  306,  126,   48,   94,  185,   37,\n",
       "         44,  437,   58], dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(Xtr_onehot_trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[0 0 0 0 0 0 0 0 0 0 0 0 0 0]'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(Xtr_onehot_trigram[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving vectors representing bigrams\n",
    "tr_file['pos_unigrams_1hot'] =  [str(x) for x in Xtr_onehot_unigram]\n",
    "tr_file['pos_bigrams_1hot'] = [str(x) for x in Xtr_onehot_bigrams]\n",
    "tr_file['pos_trigrams_1hot'] = [str(x) for x in Xtr_onehot_trigram]\n",
    "\n",
    "#saving vectors representing bigrams\n",
    "va_file['pos_unigrams_1hot'] =  [str(x) for x in Xval_onehot_unigram]\n",
    "va_file['pos_bigrams_1hot'] = [str(x) for x in Xval_onehot_bigrams]\n",
    "va_file['pos_trigrams_1hot'] = [str(x) for x in Xval_onehot_trigram]\n",
    "\n",
    "#saving vectors representing bigrams\n",
    "te_file['pos_unigrams_1hot'] =  [str(x) for x in Xte_onehot_unigram]\n",
    "te_file['pos_bigrams_1hot'] = [str(x) for x in Xte_onehot_bigrams]\n",
    "te_file['pos_trigrams_1hot'] = [str(x) for x in Xte_onehot_trigram]\n",
    "\n",
    "#saving vectors representing bigrams\n",
    "tr_file['pos_unigrams_count'] =  [str(x) for x in Xtr_count_unigram]\n",
    "tr_file['pos_bigrams_count'] = [str(x) for x in Xtr_count_bigrams]\n",
    "tr_file['pos_trigrams_count'] = [str(x) for x in Xtr_count_trigram]\n",
    "\n",
    "#saving vectors representing bigrams\n",
    "va_file['pos_unigrams_count'] =  [str(x) for x in Xval_count_unigram]\n",
    "va_file['pos_bigrams_count'] = [str(x) for x in Xval_count_bigrams]\n",
    "va_file['pos_trigrams_count'] = [str(x) for x in Xval_count_trigram]\n",
    "\n",
    "#saving vectors representing bigrams\n",
    "te_file['pos_unigrams_count'] =  [str(x) for x in Xte_count_unigram]\n",
    "te_file['pos_bigrams_count'] = [str(x) for x in Xte_count_bigrams]\n",
    "te_file['pos_trigrams_count'] = [str(x) for x in Xte_count_trigram]\n",
    "\n",
    "#saving vectors representing bigrams\n",
    "tr_file['pos_unigrams_tfidf'] =  [str(x) for x in Xtr_tfidf_unigram]\n",
    "tr_file['pos_bigrams_tfidf'] = [str(x) for x in Xtr_tfidf_bigrams]\n",
    "tr_file['pos_trigrams_tfidf'] = [str(x) for x in Xtr_tfidf_trigram]\n",
    "\n",
    "#saving vectors representing bigrams\n",
    "va_file['pos_unigrams_tfidf'] =  [str(x) for x in Xval_tfidf_unigram]\n",
    "va_file['pos_bigrams_tfidf'] = [str(x) for x in Xval_tfidf_bigrams]\n",
    "va_file['pos_trigrams_tfidf'] = [str(x) for x in Xval_tfidf_trigram]\n",
    "\n",
    "#saving vectors representing bigrams\n",
    "te_file['pos_unigrams_tfidf'] =  [str(x) for x in Xte_tfidf_unigram]\n",
    "te_file['pos_bigrams_tfidf'] = [str(x) for x in Xte_tfidf_bigrams]\n",
    "te_file['pos_trigrams_tfidf'] = [str(x) for x in Xte_tfidf_trigram]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP_Task ready to use.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'preprocessing' from 'D:\\\\UIC\\\\Fall 2018\\\\Statistical NLP\\\\Project\\\\jurat-aldo-project\\\\cs-521-project.git\\\\source\\\\CS-521-PROJECT\\\\preprocessing.py'>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing.get_keywords(tr_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing.bigphrase_tfidf_feats(tr_file[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIWC Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import mutual_info_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LIWC analysis files\n",
    "liwc_tr = pd.read_csv('..\\\\dataset\\\\{0}'.format('train_liwc.csv'))\n",
    "liwc_va = pd.read_csv('..\\\\dataset\\\\{0}'.format('valid_liwc.csv'))\n",
    "liwc_te = pd.read_csv('..\\\\dataset\\\\{0}'.format('test_liwc.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:323: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "# Scale the features so there are equally treated in terms of measure units\n",
    "scaler = MinMaxScaler()\n",
    "liwc_features_tr = scaler.fit_transform(liwc_tr.iloc[1:,3:])\n",
    "liwc_features_va = scaler.transform(liwc_va.iloc[:,14:])\n",
    "liwc_features_te = scaler.transform(liwc_te.iloc[:,14:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using MI compute the information between the variables and the target\n",
    "feature_selected = mutual_info_classif(liwc_features_tr[(tr_file['b_label']!=0)],tr_file['b_label'][(tr_file['b_label']!=0)] , random_state=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "features= liwc_tr.columns[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 WC 0.007209380888116179\n",
      "2 Clout 0.010456188432977909\n",
      "3 Authentic 0.010548039046121005\n",
      "8 function 0.006667502088415489\n",
      "12 we 0.006807810310222262\n",
      "14 shehe 0.009698999481267068\n",
      "16 ipron 0.010291456016067846\n",
      "20 adverb 0.006216147941537908\n",
      "22 negate 0.006847868219126196\n",
      "23 verb 0.00919737599536874\n",
      "25 compare 0.007334629937614245\n",
      "27 number 0.014939487362129134\n",
      "37 friend 0.005521338422770361\n",
      "42 cause 0.006021596468896551\n",
      "44 tentat 0.010037790395334367\n",
      "46 differ 0.010949844534135389\n",
      "47 percept 0.007246995579433424\n",
      "49 hear 0.013965229694862469\n",
      "50 feel 0.007178910737728694\n",
      "54 sexual 0.006709695165489027\n",
      "56 drives 0.007069115776741519\n",
      "58 achieve 0.0067695826531388725\n",
      "65 relativ 0.009136025691497629\n",
      "68 time 0.00854653048708709\n",
      "75 informal 0.006476596966411563\n",
      "76 swear 0.006526528508027152\n",
      "77 netspeak 0.005786357771375705\n",
      "91 Parenth 0.00758748597628367\n"
     ]
    }
   ],
   "source": [
    "# Extract the more relevant LIWC features\n",
    "relevant_liwc_features = list()\n",
    "for index, f in enumerate(features):\n",
    "    if feature_selected[index]>0.005:\n",
    "        relevant_liwc_features.append(f)\n",
    "        print(index, f, feature_selected[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add LIWC features to the dataframe\n",
    "for index,col in enumerate(liwc_tr.columns[3:]):\n",
    "    if col in relevant_liwc_features:\n",
    "        tr_file[col] = liwc_features_tr.T[index]\n",
    "        \n",
    "for index,col in enumerate(liwc_va.columns[14:]):\n",
    "    if col in relevant_liwc_features:\n",
    "        va_file[col] = liwc_features_va.T[index]\n",
    "        \n",
    "for index,col in enumerate(liwc_te.columns[14:]):\n",
    "    if col in relevant_liwc_features:\n",
    "        te_file[col] = liwc_features_te.T[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Blob Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from textblob import Blobber\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "from textblob.np_extractors import ConllExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractTextBlobFeatures(corpus):\n",
    "    extractor = ConllExtractor()\n",
    "    text_blob_features = np.zeros((len(corpus),4))\n",
    "    blob_sentiment_analyzer = Blobber(analyzer=NaiveBayesAnalyzer())\n",
    "    for i,each_text in enumerate(corpus):\n",
    "        #print('analyzing: ',i)\n",
    "        #blob_sentiment_analyzer = TextBlob(each_text, analyzer=NaiveBayesAnalyzer())\n",
    "        text_blob_features[i,0]=blob_sentiment_analyzer(each_text).sentiment[1]\n",
    "        text_blob_features[i,1]=blob_sentiment_analyzer(each_text).sentiment[2]\n",
    "        #text_blob_features[i,2]= TextBlob(each_text).polarity\n",
    "        text_blob_features[i,2]= TextBlob(each_text).subjectivity\n",
    "        noun_phrase_extractor = TextBlob(each_text, np_extractor=extractor)\n",
    "        text_blob_features[i,3]= len(noun_phrase_extractor.noun_phrases)\n",
    "    return text_blob_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_tb_features = extractTextBlobFeatures(tr_file['preprocessed'])\n",
    "va_tb_features = extractTextBlobFeatures(va_file['preprocessed'])\n",
    "te_tb_features = extractTextBlobFeatures(te_file['preprocessed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_feature_names = ['sentiment_pos', 'sentiment_neg','subjectivity','noun_phrases_count']\n",
    "for i in range(0,4):\n",
    "    tr_file[tb_feature_names[i]] = tr_tb_features.T[i]\n",
    "    va_file[tb_feature_names[i]] = va_tb_features.T[i]\n",
    "    te_file[tb_feature_names[i]] = te_tb_features.T[i]\n",
    "\n",
    "    \n",
    "phrases_scaler = MinMaxScaler()\n",
    "tr_file['noun_phrases_count'] = phrases_scaler.fit_transform(tr_file['noun_phrases_count'].values.reshape(-1, 1))\n",
    "va_file['noun_phrases_count']= phrases_scaler.transform(va_file['noun_phrases_count'].values.reshape(-1, 1))\n",
    "te_file['noun_phrases_count'] = phrases_scaler.transform(te_file['noun_phrases_count'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training context based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check trainig statements\n",
    "tr_file['all_cap'] = [1 if len(x)>0 else 0 for x in [re.findall('([A-Z]+\\s)', x) for x in tr_file['Statement'].values]]\n",
    "tr_file['quotation'] = [1 if len(x)>0 else 0 for x in [re.findall(r'[\"|\\']([^\"]*)[\"|\\']', x) for x in tr_file['Statement'].values]]\n",
    "tr_file['parenthesis'] = [1 if len(x)>0 else 0 for x in [re.findall(r'[(]([^\"]*)[)]', x) for x in tr_file['Statement'].values]]\n",
    "\n",
    "# Check validation statements\n",
    "va_file['all_cap'] = [1 if len(x)>0 else 0 for x in [re.findall('([A-Z]+\\s)', x) for x in va_file['Statement'].values]]\n",
    "va_file['quotation'] = [1 if len(x)>0 else 0 for x in [re.findall(r'[\"|\\']([^\"]*)[\"|\\']', x) for x in va_file['Statement'].values]]\n",
    "va_file['parenthesis'] = [1 if len(x)>0 else 0 for x in [re.findall(r'[(]([^\"]*)[)]', x) for x in va_file['Statement'].values]]\n",
    "\n",
    "# Check testing statements\n",
    "te_file['all_cap'] = [1 if len(x)>0 else 0 for x in [re.findall('([A-Z]+\\s)', x) for x in te_file['Statement'].values]]\n",
    "te_file['quotation'] = [1 if len(x)>0 else 0 for x in [re.findall(r'[\"|\\']([^\"]*)[\"|\\']', x) for x in te_file['Statement'].values]]\n",
    "te_file['parenthesis'] = [1 if len(x)>0 else 0 for x in [re.findall(r'[(]([^\"]*)[)]', x) for x in te_file['Statement'].values]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run machine learning models on feature extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords # nltk stop words\n",
    "from nltk.tokenize import word_tokenize # nltk word tokenizer\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to clean text data\n",
    "def lemmatize_remove_stop_words(corpus):\n",
    "    print('Processing data...')\n",
    "    result = list()\n",
    "    # define a lemmatizer\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    # tokenize and remove stopwords from every sentence\n",
    "    stop_words = stopwords.words('english')\n",
    "    # iterate over every sentence and apply the three filters to remove stopwords, extract lemmas for verbs and nouns\n",
    "    for index, sentence in enumerate(corpus):\n",
    "        #print('Cleaning sentence number:', index, end=\"##\")\n",
    "        #print('Making text all lower case...',end=\"--\")\n",
    "        clean_text = sentence.lower() #  make all words lower case\n",
    "        #print('Removing punctuation...', end=\"--\")\n",
    "        clean_text = re.sub(r'[{0}]'.format(string.punctuation),'',clean_text)# remove punctuation\n",
    "        #print('Removing numberic data and symbols...', end=\"--\")\n",
    "        clean_text = re.sub(r'\\w*\\d\\w*','',clean_text) # remove alpha numerics\n",
    "        #print('Splitting into tokens...', end=\"--\")\n",
    "        tokenized_sentence = word_tokenize(clean_text) # split sentence into tokens\n",
    "        #print('Lemmatizing verbs...',end=\"--\")\n",
    "        filter_one = [wordnet_lemmatizer.lemmatize(word, pos=\"v\") for word in tokenized_sentence] # lemmatize verbs\n",
    "        #print('Lemmatizing nouns...',end=\"--\")\n",
    "        filter_two = [wordnet_lemmatizer.lemmatize(word, pos=\"n\") for word in filter_one] # lemmatize nouns\n",
    "        #print('Removing stop words...',end=\"--\")\n",
    "        filter_three = [w for w in filter_two if w not in stop_words] # remove stop words\n",
    "        #print('Removing extra white spaces',end=\"--\")\n",
    "        all_clean = (' '.join(filter_three)).strip() # remove extra white spaces\n",
    "        #print('All clean for sentence: ', index,end=\"\\r\")\n",
    "        result.append(all_clean) # append data to result\n",
    "    print('Finished!')\n",
    "    return np.array(result) # convert to numpy array and return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import different models that we want to train\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "#metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "_RANDOM_STATE_V = 45\n",
    "models = [DummyClassifier,\n",
    "          DecisionTreeClassifier, \n",
    "          Perceptron, \n",
    "          LogisticRegression, \n",
    "          MultinomialNB, \n",
    "          BernoulliNB, \n",
    "          SGDClassifier, \n",
    "          SVC\n",
    "         ]#, \n",
    "          #SVC]\n",
    "defaults = [{'strategy':'most_frequent', 'random_state': _RANDOM_STATE_V}, #Baseline\n",
    "    {'max_depth': 3, 'criterion':'entropy','random_state': _RANDOM_STATE_V}, #DT\n",
    "            {'penalty':'l2','early_stopping': True,'random_state': _RANDOM_STATE_V}, #Perceptron\n",
    "            {'penalty':'l2','tol':0.0001, 'C':1.0,'max_iter':100,'random_state': _RANDOM_STATE_V}, #Linear Regression\n",
    "            {}, #MultinomialNB\n",
    "            {}, #BernoulliNB\n",
    "            {'loss':'hinge', 'penalty':'l2', 'alpha':0.0001,'random_state': _RANDOM_STATE_V}, #SGDClassifier,\n",
    "            {'C':1.0, 'kernel':'linear', 'degree':3, 'gamma':1,'random_state': _RANDOM_STATE_V}\n",
    "            #{'n_neighbors':5},\n",
    "           #{}\n",
    "           ]#, #KNeighborsClassifier\n",
    "            #{'C':1.0, 'kernel':'rbf', 'degree':3, 'gamma':1,'random_state': _RANDOM_STATE_V}, #SVC]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making string of the data\n",
    "training_str = [\" \".join(x) for x in removed_pos_bigrams_p]\n",
    "validation_str = [\" \".join(x) for x in removed_pos_bigrams_va]\n",
    "\n",
    "#replace $ by dollar\n",
    "training_str = [x.replace('$', 'dollar').replace('<s>','') for x in training_str]\n",
    "validation_str = [x.replace('$', 'dollar').replace('<s>','') for x in validation_str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       We have less Americans working now than in the...\n",
       "1       When Obama was sworn into office he DID NOT us...\n",
       "2       Says Having organizations parading as being so...\n",
       "3       Says nearly half of Oregons children are poor ...\n",
       "4       On attacks by Republicans that various program...\n",
       "5       Says when armed civilians stop mass shootings ...\n",
       "6       Says Tennessee is providing millions of dollar...\n",
       "7       The health care reform plan would set limits s...\n",
       "8       Says Donald Trump started his career back in 1...\n",
       "9       Bill White has a long history of trying to lim...\n",
       "10      John McCains chief economic adviser during the...\n",
       "11      Says 21000 Wisconsin residents got jobs in 201...\n",
       "12      State revenue projections have missed the mark...\n",
       "13      The median income of a middle class family wen...\n",
       "14      Every citizen is entitled to the freedom of sp...\n",
       "15      Rick Perry has advocated abandoning Social Sec...\n",
       "16      Two thirds to three quarters of people without...\n",
       "17      Congress has spent 66 of the first 100 days of...\n",
       "18      Mark Sharpe has lowered property taxes by 17 p...\n",
       "19      Says Iowa Gov Terry Branstad chartered a plane...\n",
       "20      If you dont buy cigarettes at your local super...\n",
       "21      Says President Barack Obama has said that ever...\n",
       "22      Georgia has had more bank failures than any o...\n",
       "23      Bank of America could create 878300 jobs with ...\n",
       "24      Thom Tillis cut almost 500 million from educat...\n",
       "25      If people work and make more money they lose m...\n",
       "26      We are poised to get rid of over 1000 more reg...\n",
       "27      A flight from Atlanta to Houston was canceled ...\n",
       "28      Administrative employees at colleges and unive...\n",
       "29      Private prison systems are calculating how man...\n",
       "                              ...                        \n",
       "1254    Says Ron Saunders made the choice to stand wit...\n",
       "1255    Since President Obama took office there are ov...\n",
       "1256    People in the top tax bracket these socalled w...\n",
       "1257    Barack Obama has pledged to reduce the size of...\n",
       "1258    Large majorities of the public oppose major ch...\n",
       "1259     Fidel Castro endorses Obama _NNP NNP_VBZ VBZ_NNP\n",
       "1260    Says spending in the fiscal 2009 budget was lo...\n",
       "1261    There is no statistical evidence that bail bon...\n",
       "1262    Household incomes are down more than 4000 sinc...\n",
       "1263    The US is spending one out of every six Defens...\n",
       "1264    Says personhood legislation she sponsored woul...\n",
       "1265    The Republican Party fought very hard in the 6...\n",
       "1266    A major part of the climate change bill sponso...\n",
       "1267    Ohios credit rating is as high as you can get ...\n",
       "1268    Says every day of a special session costs taxp...\n",
       "1269    Weve excluded lobbyists from policymaking jobs...\n",
       "1270    The federal deficit would pay for 40 gallons o...\n",
       "1271    Says George Flinn has been a no show at schedu...\n",
       "1272    Social Security and other federal checks may n...\n",
       "1273    TSA started off with 16500 screeners and the a...\n",
       "1274    The proposed excise tax on Cadillac health pla...\n",
       "1275    I supported the surge I argued for it Im the o...\n",
       "1276    Says US Rep Charles Bass wants to privatize So...\n",
       "1277    More Americans believe the moon landing was fa...\n",
       "1278    In the past two years Democrats have spent mor...\n",
       "1279    For the first time in more than a decade impor...\n",
       "1280    Says Donald Trump has bankrupted his companies...\n",
       "1281    John McCain and George Bush have absolutely no...\n",
       "1282    A new poll shows 62 percent support the presid...\n",
       "1283    No one claims the report vindicating New Jerse...\n",
       "Name: preprocessed, Length: 1284, dtype: object"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "va_file['preprocessed']+' '+validation_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DummyClassifier\n",
      "Accuracy:  0.5945945945945946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00         0\n",
      "           1       1.00      0.59      0.75      1036\n",
      "\n",
      "   micro avg       0.59      0.59      0.59      1036\n",
      "   macro avg       0.50      0.30      0.37      1036\n",
      "weighted avg       1.00      0.59      0.75      1036\n",
      "\n",
      "Training DecisionTreeClassifier\n",
      "Accuracy:  0.6138996138996139\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.44      0.53      0.48       352\n",
      "           1       0.73      0.66      0.69       684\n",
      "\n",
      "   micro avg       0.61      0.61      0.61      1036\n",
      "   macro avg       0.59      0.59      0.59      1036\n",
      "weighted avg       0.63      0.61      0.62      1036\n",
      "\n",
      "Training Perceptron\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in Perceptron in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.5743243243243243\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.38      0.47      0.42       341\n",
      "           1       0.71      0.63      0.66       695\n",
      "\n",
      "   micro avg       0.57      0.57      0.57      1036\n",
      "   macro avg       0.54      0.55      0.54      1036\n",
      "weighted avg       0.60      0.57      0.58      1036\n",
      "\n",
      "Training LogisticRegression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6264478764478765\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.42      0.55      0.48       323\n",
      "           1       0.76      0.66      0.71       713\n",
      "\n",
      "   micro avg       0.63      0.63      0.63      1036\n",
      "   macro avg       0.59      0.61      0.59      1036\n",
      "weighted avg       0.66      0.63      0.64      1036\n",
      "\n",
      "Training MultinomialNB\n",
      "Accuracy:  0.6361003861003861\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.37      0.58      0.45       267\n",
      "           1       0.82      0.66      0.73       769\n",
      "\n",
      "   micro avg       0.64      0.64      0.64      1036\n",
      "   macro avg       0.59      0.62      0.59      1036\n",
      "weighted avg       0.70      0.64      0.66      1036\n",
      "\n",
      "Training BernoulliNB\n",
      "Accuracy:  0.6254826254826255\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.48      0.54      0.51       370\n",
      "           1       0.73      0.67      0.70       666\n",
      "\n",
      "   micro avg       0.63      0.63      0.63      1036\n",
      "   macro avg       0.60      0.61      0.60      1036\n",
      "weighted avg       0.64      0.63      0.63      1036\n",
      "\n",
      "Training SGDClassifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6177606177606177\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.31      0.55      0.39       234\n",
      "           1       0.83      0.64      0.72       802\n",
      "\n",
      "   micro avg       0.62      0.62      0.62      1036\n",
      "   macro avg       0.57      0.59      0.56      1036\n",
      "weighted avg       0.71      0.62      0.65      1036\n",
      "\n",
      "Training SVC\n",
      "Accuracy:  0.6225868725868726\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.23      0.59      0.33       163\n",
      "           1       0.89      0.63      0.74       873\n",
      "\n",
      "   micro avg       0.62      0.62      0.62      1036\n",
      "   macro avg       0.56      0.61      0.53      1036\n",
      "weighted avg       0.79      0.62      0.67      1036\n",
      "\n",
      "DummyClassifier,0.5945945945945946\n",
      "DecisionTreeClassifier,0.6138996138996139\n",
      "Perceptron,0.5743243243243243\n",
      "LogisticRegression,0.6264478764478765\n",
      "MultinomialNB,0.6361003861003861\n",
      "BernoulliNB,0.6254826254826255\n",
      "SGDClassifier,0.6177606177606177\n",
      "SVC,0.6225868725868726\n"
     ]
    }
   ],
   "source": [
    "#Xtr = lemmatize_remove_stop_words(tr_file['preprocessed'][(tr_file['b_label']!=0)])\n",
    "#Xde = lemmatize_remove_stop_words(va_file['preprocessed'][(va_file['b_label']!=0)])\n",
    "#Xtr = tr_file['preprocessed'][(tr_file['b_label']!=0)]\n",
    "#Xde = va_file['preprocessed'][(va_file['b_label']!=0)]\n",
    "#Xtr = tr_file['preprocessed'][(tr_file['b_label']!=0)]\n",
    "#Xde = va_file['preprocessed'][(va_file['b_label']!=0)]\n",
    "#Ytr = tr_file['b_label'][(tr_file['b_label']!=0)]\n",
    "#Yde = va_file['b_label'][(va_file['b_label']!=0)]\n",
    "#Xtr= Xtr_onehot_unigram[(tr_file['b_label']!=0)]\n",
    "#Xde= Xval_onehot_unigram[(va_file['b_label']!=0)]\n",
    "#Xtr = Xtr_onehot_bigrams#tr_tb_features#liwc_features_tr#tr_file['preprocessed']+' '+training_str\n",
    "#Xde = Xval_onehot_bigrams#va_tb_features#liwc_features_va#va_file['preprocessed']+' '+validation_str\n",
    "Ytr = tr_file['b_label'][(tr_file['b_label']!=0)][(tr_file['b_label']!=0)]\n",
    "Yde = va_file['b_label'][(va_file['b_label']!=0)]\n",
    "\n",
    "report_list = list()\n",
    "for index, model in enumerate(models):\n",
    "    text_clf = Pipeline([\n",
    "        #('vect', CountVectorizer()),\n",
    "        #('tfidf', TfidfTransformer()),\n",
    "        ('clf', model(**defaults[index])),\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    #text_clf = model(**defaults[index])\n",
    "    print('Training {0}'.format(model.__name__))\n",
    "    text_clf.fit(Xtr, Ytr)\n",
    "    pred = text_clf.predict(Xde)\n",
    "    #print('Accuracy: ', np.mean(pred==Yde))\n",
    "    print('Accuracy: ', accuracy_score(pred,Yde))\n",
    "    print(classification_report(pred, Yde))\n",
    "    #report_list.append((model.__name__, np.mean(pred==Yde), f1_score(pred, Yde, average='binary', pos_label=1),f1_score(pred, Yde, average='binary', pos_label=-1)))\n",
    "    report_list.append((model.__name__, accuracy_score(pred,Yde)))\n",
    "for e in report_list:\n",
    "    #print('{0},{1},{2}'.format(e[1], e[2], e[3]))\n",
    "    print('{0},{1}'.format(e[0], e[1]))                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'len(Xtr_combined[0])'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"len(Xtr_combined[0])\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#penalty=l2, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, \\n#random_state=None, solver=warn, max_iter=1000, multi_class=warn, verbose=0, warm_start=False, n_jobs=None\\n\\nparameters = {\\n    #\\'vect__max_df\\': (0.5, 0.75, 1.0),\\n    #\\'vect__max_features\\': (None, 5000, 10000, 50000),\\n    #\\'vect__ngram_range\\': ((1, 1), (1, 2)),  # unigrams or bigrams\\n    #\\'tfidf__use_idf\\': (True, False),\\n    #\\'tfidf__norm\\': (\\'l1\\', \\'l2\\'),\\n    #\\'clf__max_iter\\': (10,20,30,50,100),\\n    #\\'clf__alpha\\': (1e-5,1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1),\\n    #\\'clf__penalty\\': \\'l2\\',\\n    \\'clf__penalty\\': (\\'l2\\',\\'l1\\'),\\n    # \\'clf__max_iter\\': (10, 50, 80),\\n    #alpha=1e-5,\\n    \\'clf__C\\' :(0.01,0.05,0.1,0.2,0.3,0.4,0.5,1,5,8,12) ,\\n    #\\'clf__class_weight\\' : ({1:1},{1:2},{1:3},{1:4},{1:5}\\n    #\\'clf__alpha\\':(0.2,0.5,1,2,3,10,15,25,50)\\n    \\n    #\\'clf__loss\\': \\'hinge\\' #, \\'log\\', \\'modified_huber\\', \\'squared_hinge\\', \\'perceptron\\'\\n}\\n\\ntext_clf = Pipeline([\\n    (\\'vect\\', CountVectorizer()),\\n    (\\'tfidf\\', TfidfTransformer()),\\n    (\\'clf\\', LogisticRegression(penalty=\\'l2\\', random_state=_RANDOM_STATE_V, max_iter=1000)),\\n])\\n\\ngrid_search = GridSearchCV(text_clf, parameters, cv=5,n_jobs=1, verbose=1, scoring=\\'accuracy\\')\\n\\nprint(\"Performing grid search...\")\\nprint(\"pipeline:\", [name for name, _ in text_clf.steps])\\nprint(\"parameters:\")\\nprint(parameters)\\n#validation_set = PredefinedSplit(test_fold=validation_text)\\ngrid_search.fit(Xtr, Ytr)\\nprint(\"Best score: %0.3f\" % grid_search.best_score_)\\nprint(\"Best parameters set:\")\\nbest_parameters = grid_search.best_estimator_.get_params()\\nfor param_name in sorted(parameters.keys()):\\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\\n\\n#print(\\'Training {0}\\'.format(SGDClassifier.__name__))\\n#text_clf.fit(X_train_oversamples, Y_train_resampled)\\n#pred = text_clf.predict(validation_text)\\n#print(classification_report(pred, validation_target))'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"#penalty=l2, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, \n",
    "#random_state=None, solver=warn, max_iter=1000, multi_class=warn, verbose=0, warm_start=False, n_jobs=None\n",
    "\n",
    "parameters = {\n",
    "    #'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    #'vect__max_features': (None, 5000, 10000, 50000),\n",
    "    #'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    #'tfidf__use_idf': (True, False),\n",
    "    #'tfidf__norm': ('l1', 'l2'),\n",
    "    #'clf__max_iter': (10,20,30,50,100),\n",
    "    #'clf__alpha': (1e-5,1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1),\n",
    "    #'clf__penalty': 'l2',\n",
    "    'clf__penalty': ('l2','l1'),\n",
    "    # 'clf__max_iter': (10, 50, 80),\n",
    "    #alpha=1e-5,\n",
    "    'clf__C' :(0.01,0.05,0.1,0.2,0.3,0.4,0.5,1,5,8,12) ,\n",
    "    #'clf__class_weight' : ({1:1},{1:2},{1:3},{1:4},{1:5}\n",
    "    #'clf__alpha':(0.2,0.5,1,2,3,10,15,25,50)\n",
    "    \n",
    "    #'clf__loss': 'hinge' #, 'log', 'modified_huber', 'squared_hinge', 'perceptron'\n",
    "}\n",
    "\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', LogisticRegression(penalty='l2', random_state=_RANDOM_STATE_V, max_iter=1000)),\n",
    "])\n",
    "\n",
    "grid_search = GridSearchCV(text_clf, parameters, cv=5,n_jobs=1, verbose=1, scoring='accuracy')\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in text_clf.steps])\n",
    "print(\"parameters:\")\n",
    "print(parameters)\n",
    "#validation_set = PredefinedSplit(test_fold=validation_text)\n",
    "grid_search.fit(Xtr, Ytr)\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "#print('Training {0}'.format(SGDClassifier.__name__))\n",
    "#text_clf.fit(X_train_oversamples, Y_train_resampled)\n",
    "#pred = text_clf.predict(validation_text)\n",
    "#print(classification_report(pred, validation_target))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"text_clf = Pipeline([\\n    ('vect', CountVectorizer()),\\n    ('tfidf', TfidfTransformer()),\\n    ('clf', LogisticRegression(penalty='l2', random_state=_RANDOM_STATE_V, max_iter=1000, C=0.5)),\\n])\\n\\ntext_clf.fit(Xtr,Ytr)\\npred = text_clf.predict(Xde)\\nprint('Accuracy: ', accuracy_score(pred, Yde))\""
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', LogisticRegression(penalty='l2', random_state=_RANDOM_STATE_V, max_iter=1000, C=0.5)),\n",
    "])\n",
    "\n",
    "text_clf.fit(Xtr,Ytr)\n",
    "pred = text_clf.predict(Xde)\n",
    "print('Accuracy: ', accuracy_score(pred, Yde))\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adj Phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import spacy\\nimport en_core_web_sm\\nnlp=en_core_web_sm.load()'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import spacy\n",
    "import en_core_web_sm\n",
    "nlp=en_core_web_sm.load()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def get_noun_adj_pairs(corpus):\\n    result = list()\\n    for c in corpus:\\n        parsed=nlp(c)\\n        noun_adj_pairs_res=[]\\n        for i, tok in enumerate(parsed):\\n            if tok.pos_ not in ('NOUN','PRON'):\\n                continue\\n            for j in range(i+1, len(parsed)):\\n                if parsed[j].pos_=='ADJ':\\n                    noun_adj_pairs_res.append((tok, parsed[j]))\\n                    break\\n        result.append(noun_adj_pairs_res)\\n    return result\""
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def get_noun_adj_pairs(corpus):\n",
    "    result = list()\n",
    "    for c in corpus:\n",
    "        parsed=nlp(c)\n",
    "        noun_adj_pairs_res=[]\n",
    "        for i, tok in enumerate(parsed):\n",
    "            if tok.pos_ not in ('NOUN','PRON'):\n",
    "                continue\n",
    "            for j in range(i+1, len(parsed)):\n",
    "                if parsed[j].pos_=='ADJ':\n",
    "                    noun_adj_pairs_res.append((tok, parsed[j]))\n",
    "                    break\n",
    "        result.append(noun_adj_pairs_res)\n",
    "    return result\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Xtr[5]'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Xtr[5]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"get_noun_adj_pairs(tr_file['preprocessed'])\""
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"get_noun_adj_pairs(tr_file['preprocessed'])\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tokenizer(num_words=5000) # create tokenizer with a max number of words to take into account according to frequency\n",
    "t.fit_on_texts(tr_file['preprocessed'][(tr_file['b_label']!=0)]) # fit tokenizer with train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters:\n",
    "max_features = len(t.word_index) + 1 \n",
    "maxlen = 300\n",
    "batch_size = 32\n",
    "embedding_dims = 100\n",
    "filters = 120\n",
    "kernel_size = 5\n",
    "hidden_dims = 200\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = t.texts_to_sequences(tr_file['preprocessed'][(tr_file['b_label']!=0)])\n",
    "X_dev = t.texts_to_sequences(va_file['preprocessed'][(va_file['b_label']!=0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train,maxlen=maxlen,padding='post')\n",
    "X_dev = pad_sequences(X_dev, maxlen=maxlen, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.sequence import pad_sequences # To make vectors the same size. \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "174"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 300, 100)          1205800   \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 296, 120)          60120     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_5 (Glob (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 200)               24200     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 1,290,321\n",
      "Trainable params: 1,290,321\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Simple one hidden layer NN with a Convolutional layer for filtering and GlobalMaxPooling 1D \n",
    "model.add(Embedding(input_dim=max_features, \n",
    "                           output_dim=embedding_dims, \n",
    "                           input_length=maxlen))\n",
    "model.add(Conv1D(filters=filters, kernel_size=kernel_size, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(hidden_dims, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8126 samples, validate on 1036 samples\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6432/8126 [======================>.......] - ETA: 2:06 - loss: 0.6801 - acc: 0.062 - ETA: 1:13 - loss: 0.6775 - acc: 0.031 - ETA: 55s - loss: 0.6660 - acc: 0.020 - ETA: 47s - loss: 0.6377 - acc: 0.01 - ETA: 42s - loss: 0.6324 - acc: 0.01 - ETA: 39s - loss: 0.6135 - acc: 0.01 - ETA: 37s - loss: 0.6066 - acc: 0.00 - ETA: 35s - loss: 0.5848 - acc: 0.00 - ETA: 33s - loss: 0.5748 - acc: 0.00 - ETA: 32s - loss: 0.5893 - acc: 0.00 - ETA: 31s - loss: 0.5509 - acc: 0.00 - ETA: 30s - loss: 0.5204 - acc: 0.00 - ETA: 29s - loss: 0.5059 - acc: 0.00 - ETA: 28s - loss: 0.5044 - acc: 0.00 - ETA: 28s - loss: 0.4778 - acc: 0.00 - ETA: 27s - loss: 0.4854 - acc: 0.00 - ETA: 27s - loss: 0.4728 - acc: 0.00 - ETA: 26s - loss: 0.4259 - acc: 0.00 - ETA: 26s - loss: 0.3931 - acc: 0.00 - ETA: 26s - loss: 0.3915 - acc: 0.00 - ETA: 25s - loss: 0.3589 - acc: 0.00 - ETA: 25s - loss: 0.3362 - acc: 0.00 - ETA: 25s - loss: 0.3589 - acc: 0.00 - ETA: 25s - loss: 0.3694 - acc: 0.00 - ETA: 24s - loss: 0.3641 - acc: 0.00 - ETA: 24s - loss: 0.3673 - acc: 0.00 - ETA: 24s - loss: 0.3627 - acc: 0.00 - ETA: 23s - loss: 0.3664 - acc: 0.00 - ETA: 23s - loss: 0.3780 - acc: 0.00 - ETA: 23s - loss: 0.3578 - acc: 0.00 - ETA: 23s - loss: 0.3470 - acc: 0.00 - ETA: 23s - loss: 0.3690 - acc: 0.00 - ETA: 22s - loss: 0.3712 - acc: 0.00 - ETA: 22s - loss: 0.3377 - acc: 0.00 - ETA: 22s - loss: 0.2967 - acc: 0.00 - ETA: 22s - loss: 0.2989 - acc: 0.00 - ETA: 22s - loss: 0.3200 - acc: 0.00 - ETA: 22s - loss: 0.3138 - acc: 0.00 - ETA: 21s - loss: 0.3220 - acc: 0.00 - ETA: 21s - loss: 0.3280 - acc: 0.00 - ETA: 21s - loss: 0.3303 - acc: 0.00 - ETA: 21s - loss: 0.3482 - acc: 0.00 - ETA: 21s - loss: 0.3491 - acc: 0.00 - ETA: 21s - loss: 0.3407 - acc: 0.00 - ETA: 20s - loss: 0.3248 - acc: 0.00 - ETA: 20s - loss: 0.3260 - acc: 0.00 - ETA: 20s - loss: 0.3267 - acc: 0.00 - ETA: 20s - loss: 0.3169 - acc: 0.00 - ETA: 20s - loss: 0.3127 - acc: 0.00 - ETA: 20s - loss: 0.3040 - acc: 0.00 - ETA: 20s - loss: 0.2963 - acc: 0.00 - ETA: 20s - loss: 0.2929 - acc: 0.00 - ETA: 19s - loss: 0.3017 - acc: 0.00 - ETA: 19s - loss: 0.3078 - acc: 0.00 - ETA: 19s - loss: 0.2987 - acc: 0.00 - ETA: 19s - loss: 0.3007 - acc: 0.00 - ETA: 19s - loss: 0.2958 - acc: 0.00 - ETA: 19s - loss: 0.2858 - acc: 0.00 - ETA: 19s - loss: 0.3022 - acc: 0.00 - ETA: 19s - loss: 0.3124 - acc: 0.00 - ETA: 19s - loss: 0.3023 - acc: 0.00 - ETA: 18s - loss: 0.2995 - acc: 0.00 - ETA: 18s - loss: 0.2998 - acc: 9.9206e- - ETA: 18s - loss: 0.3106 - acc: 9.7656e- - ETA: 18s - loss: 0.3225 - acc: 9.6154e- - ETA: 18s - loss: 0.3186 - acc: 9.4697e- - ETA: 18s - loss: 0.3237 - acc: 9.3284e- - ETA: 18s - loss: 0.3116 - acc: 9.1912e- - ETA: 18s - loss: 0.3100 - acc: 9.0580e- - ETA: 17s - loss: 0.3070 - acc: 8.9286e- - ETA: 17s - loss: 0.3079 - acc: 8.8028e- - ETA: 17s - loss: 0.2990 - acc: 8.6806e- - ETA: 17s - loss: 0.2922 - acc: 8.5616e- - ETA: 17s - loss: 0.2897 - acc: 8.4459e- - ETA: 17s - loss: 0.2959 - acc: 8.3333e- - ETA: 17s - loss: 0.2900 - acc: 8.2237e- - ETA: 17s - loss: 0.2912 - acc: 8.1169e- - ETA: 17s - loss: 0.2784 - acc: 8.0128e- - ETA: 17s - loss: 0.2875 - acc: 7.9114e- - ETA: 16s - loss: 0.2873 - acc: 7.8125e- - ETA: 16s - loss: 0.2855 - acc: 7.7160e- - ETA: 16s - loss: 0.2945 - acc: 7.6220e- - ETA: 16s - loss: 0.2946 - acc: 7.5301e- - ETA: 16s - loss: 0.2993 - acc: 7.4405e- - ETA: 16s - loss: 0.3041 - acc: 7.3529e- - ETA: 16s - loss: 0.3039 - acc: 7.2674e- - ETA: 16s - loss: 0.3064 - acc: 7.1839e- - ETA: 16s - loss: 0.3048 - acc: 7.1023e- - ETA: 15s - loss: 0.3066 - acc: 7.0225e- - ETA: 15s - loss: 0.3117 - acc: 6.9444e- - ETA: 15s - loss: 0.3004 - acc: 6.8681e- - ETA: 15s - loss: 0.3035 - acc: 6.7935e- - ETA: 15s - loss: 0.3026 - acc: 6.7204e- - ETA: 15s - loss: 0.3028 - acc: 6.6489e- - ETA: 15s - loss: 0.2954 - acc: 6.5789e- - ETA: 15s - loss: 0.2965 - acc: 6.5104e- - ETA: 15s - loss: 0.2880 - acc: 6.4433e- - ETA: 15s - loss: 0.2853 - acc: 6.3776e- - ETA: 15s - loss: 0.2897 - acc: 6.3131e- - ETA: 14s - loss: 0.2993 - acc: 6.2500e- - ETA: 14s - loss: 0.3056 - acc: 6.1881e- - ETA: 14s - loss: 0.3057 - acc: 6.1275e- - ETA: 14s - loss: 0.3058 - acc: 6.0680e- - ETA: 14s - loss: 0.3089 - acc: 6.0096e- - ETA: 14s - loss: 0.3088 - acc: 5.9524e- - ETA: 14s - loss: 0.2966 - acc: 5.8962e- - ETA: 14s - loss: 0.2934 - acc: 5.8411e- - ETA: 14s - loss: 0.2880 - acc: 5.7870e- - ETA: 13s - loss: 0.2913 - acc: 5.7339e- - ETA: 13s - loss: 0.2926 - acc: 5.6818e- - ETA: 13s - loss: 0.2843 - acc: 5.6306e- - ETA: 13s - loss: 0.2908 - acc: 5.5804e- - ETA: 13s - loss: 0.2893 - acc: 5.5310e- - ETA: 13s - loss: 0.2777 - acc: 5.4825e- - ETA: 13s - loss: 0.2845 - acc: 5.4348e- - ETA: 13s - loss: 0.2832 - acc: 5.3879e- - ETA: 13s - loss: 0.2840 - acc: 5.3419e- - ETA: 13s - loss: 0.2809 - acc: 5.2966e- - ETA: 12s - loss: 0.2750 - acc: 5.2521e- - ETA: 12s - loss: 0.2871 - acc: 5.2083e- - ETA: 12s - loss: 0.2924 - acc: 5.1653e- - ETA: 12s - loss: 0.2902 - acc: 5.1230e- - ETA: 12s - loss: 0.2924 - acc: 5.0813e- - ETA: 12s - loss: 0.2940 - acc: 5.0403e- - ETA: 12s - loss: 0.2892 - acc: 5.0000e- - ETA: 12s - loss: 0.2902 - acc: 4.9603e- - ETA: 12s - loss: 0.2911 - acc: 4.9213e- - ETA: 12s - loss: 0.2845 - acc: 4.8828e- - ETA: 11s - loss: 0.2847 - acc: 4.8450e- - ETA: 11s - loss: 0.2918 - acc: 4.8077e- - ETA: 11s - loss: 0.2846 - acc: 4.7710e- - ETA: 11s - loss: 0.2793 - acc: 4.7348e- - ETA: 11s - loss: 0.2781 - acc: 4.6992e- - ETA: 11s - loss: 0.2768 - acc: 4.6642e- - ETA: 11s - loss: 0.2795 - acc: 4.6296e- - ETA: 11s - loss: 0.2746 - acc: 4.5956e- - ETA: 11s - loss: 0.2716 - acc: 4.5620e- - ETA: 11s - loss: 0.2634 - acc: 4.5290e- - ETA: 11s - loss: 0.2586 - acc: 4.4964e- - ETA: 10s - loss: 0.2575 - acc: 4.4643e- - ETA: 10s - loss: 0.2532 - acc: 4.4326e- - ETA: 10s - loss: 0.2437 - acc: 4.4014e- - ETA: 10s - loss: 0.2367 - acc: 4.3706e- - ETA: 10s - loss: 0.2325 - acc: 4.3403e- - ETA: 10s - loss: 0.2372 - acc: 4.3103e- - ETA: 10s - loss: 0.2354 - acc: 4.2808e- - ETA: 10s - loss: 0.2279 - acc: 4.2517e- - ETA: 10s - loss: 0.2228 - acc: 4.2230e- - ETA: 10s - loss: 0.2274 - acc: 4.1946e- - ETA: 9s - loss: 0.2213 - acc: 4.1667e-04 - ETA: 9s - loss: 0.2179 - acc: 4.1391e-0 - ETA: 9s - loss: 0.2218 - acc: 4.1118e-0 - ETA: 9s - loss: 0.2120 - acc: 4.0850e-0 - ETA: 9s - loss: 0.2224 - acc: 4.0584e-0 - ETA: 9s - loss: 0.2095 - acc: 4.0323e-0 - ETA: 9s - loss: 0.2127 - acc: 4.0064e-0 - ETA: 9s - loss: 0.2166 - acc: 3.9809e-0 - ETA: 9s - loss: 0.2166 - acc: 3.9557e-0 - ETA: 9s - loss: 0.2045 - acc: 3.9308e-0 - ETA: 9s - loss: 0.2052 - acc: 3.9063e-0 - ETA: 8s - loss: 0.2019 - acc: 3.8820e-0 - ETA: 8s - loss: 0.1922 - acc: 3.8580e-0 - ETA: 8s - loss: 0.1928 - acc: 3.8344e-0 - ETA: 8s - loss: 0.1971 - acc: 3.8110e-0 - ETA: 8s - loss: 0.2065 - acc: 3.7879e-0 - ETA: 8s - loss: 0.2102 - acc: 3.7651e-0 - ETA: 8s - loss: 0.2016 - acc: 3.7425e-0 - ETA: 8s - loss: 0.1947 - acc: 3.7202e-0 - ETA: 8s - loss: 0.1935 - acc: 3.6982e-0 - ETA: 8s - loss: 0.2006 - acc: 3.6765e-0 - ETA: 7s - loss: 0.1947 - acc: 3.6550e-0 - ETA: 7s - loss: 0.2014 - acc: 3.6337e-0 - ETA: 7s - loss: 0.1947 - acc: 3.6127e-0 - ETA: 7s - loss: 0.1984 - acc: 3.5920e-0 - ETA: 7s - loss: 0.2036 - acc: 3.5714e-0 - ETA: 7s - loss: 0.2107 - acc: 3.5511e-0 - ETA: 7s - loss: 0.2063 - acc: 3.5311e-0 - ETA: 7s - loss: 0.1953 - acc: 3.5112e-0 - ETA: 7s - loss: 0.1924 - acc: 6.9832e-0 - ETA: 7s - loss: 0.1920 - acc: 0.0012    - ETA: 6s - loss: 0.1837 - acc: 0.001 - ETA: 6s - loss: 0.1787 - acc: 0.001 - ETA: 6s - loss: 0.1744 - acc: 0.002 - ETA: 6s - loss: 0.1720 - acc: 0.002 - ETA: 6s - loss: 0.1621 - acc: 0.003 - ETA: 6s - loss: 0.1663 - acc: 0.003 - ETA: 6s - loss: 0.1655 - acc: 0.004 - ETA: 6s - loss: 0.1613 - acc: 0.005 - ETA: 6s - loss: 0.1463 - acc: 0.005 - ETA: 6s - loss: 0.1437 - acc: 0.005 - ETA: 6s - loss: 0.1345 - acc: 0.006 - ETA: 5s - loss: 0.1323 - acc: 0.006 - ETA: 5s - loss: 0.1229 - acc: 0.007 - ETA: 5s - loss: 0.1342 - acc: 0.007 - ETA: 5s - loss: 0.1323 - acc: 0.007 - ETA: 5s - loss: 0.1288 - acc: 0.008 - ETA: 5s - loss: 0.1317 - acc: 0.008 - ETA: 5s - loss: 0.1353 - acc: 0.009 - ETA: 5s - loss: 0.1413 - acc: 0.009 - ETA: 5s - loss: 0.1256 - acc: 0.009 - ETA: 5s - loss: 0.1155 - acc: 0.0101\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8126/8126 [==============================] - ETA: 4s - loss: 0.1174 - acc: 0.010 - ETA: 4s - loss: 0.1129 - acc: 0.011 - ETA: 4s - loss: 0.1164 - acc: 0.011 - ETA: 4s - loss: 0.1093 - acc: 0.013 - ETA: 4s - loss: 0.1122 - acc: 0.013 - ETA: 4s - loss: 0.1090 - acc: 0.014 - ETA: 4s - loss: 0.1117 - acc: 0.014 - ETA: 4s - loss: 0.1170 - acc: 0.015 - ETA: 4s - loss: 0.1136 - acc: 0.016 - ETA: 4s - loss: 0.1128 - acc: 0.017 - ETA: 4s - loss: 0.1063 - acc: 0.018 - ETA: 3s - loss: 0.1002 - acc: 0.018 - ETA: 3s - loss: 0.1009 - acc: 0.019 - ETA: 3s - loss: 0.0986 - acc: 0.020 - ETA: 3s - loss: 0.0898 - acc: 0.020 - ETA: 3s - loss: 0.0916 - acc: 0.021 - ETA: 3s - loss: 0.0782 - acc: 0.021 - ETA: 3s - loss: 0.0777 - acc: 0.022 - ETA: 3s - loss: 0.0725 - acc: 0.022 - ETA: 3s - loss: 0.0735 - acc: 0.023 - ETA: 3s - loss: 0.0608 - acc: 0.024 - ETA: 2s - loss: 0.0592 - acc: 0.024 - ETA: 2s - loss: 0.0595 - acc: 0.025 - ETA: 2s - loss: 0.0532 - acc: 0.026 - ETA: 2s - loss: 0.0620 - acc: 0.026 - ETA: 2s - loss: 0.0622 - acc: 0.027 - ETA: 2s - loss: 0.0603 - acc: 0.028 - ETA: 2s - loss: 0.0497 - acc: 0.028 - ETA: 2s - loss: 0.0437 - acc: 0.029 - ETA: 2s - loss: 0.0443 - acc: 0.030 - ETA: 2s - loss: 0.0356 - acc: 0.031 - ETA: 2s - loss: 0.0315 - acc: 0.031 - ETA: 1s - loss: 0.0282 - acc: 0.031 - ETA: 1s - loss: 0.0197 - acc: 0.032 - ETA: 1s - loss: 0.0155 - acc: 0.032 - ETA: 1s - loss: 0.0151 - acc: 0.032 - ETA: 1s - loss: 0.0055 - acc: 0.032 - ETA: 1s - loss: 0.0091 - acc: 0.033 - ETA: 1s - loss: 0.0169 - acc: 0.033 - ETA: 1s - loss: 0.0134 - acc: 0.033 - ETA: 1s - loss: 0.0046 - acc: 0.033 - ETA: 1s - loss: -0.0130 - acc: 0.03 - ETA: 0s - loss: -0.0212 - acc: 0.03 - ETA: 0s - loss: -0.0182 - acc: 0.03 - ETA: 0s - loss: -0.0198 - acc: 0.03 - ETA: 0s - loss: -0.0155 - acc: 0.03 - ETA: 0s - loss: -0.0164 - acc: 0.03 - ETA: 0s - loss: -0.0248 - acc: 0.03 - ETA: 0s - loss: -0.0232 - acc: 0.03 - ETA: 0s - loss: -0.0179 - acc: 0.03 - ETA: 0s - loss: -0.0305 - acc: 0.03 - ETA: 0s - loss: -0.0389 - acc: 0.03 - 25s 3ms/step - loss: -0.0375 - acc: 0.0375 - val_loss: -0.3955 - val_acc: 0.1458\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6624/8126 [=======================>......] - ETA: 22s - loss: -4.5789 - acc: 0.093 - ETA: 23s - loss: -4.4461 - acc: 0.109 - ETA: 22s - loss: -3.8782 - acc: 0.145 - ETA: 22s - loss: -3.3078 - acc: 0.140 - ETA: 22s - loss: -3.1322 - acc: 0.143 - ETA: 22s - loss: -3.6263 - acc: 0.130 - ETA: 23s - loss: -3.5342 - acc: 0.147 - ETA: 22s - loss: -3.1707 - acc: 0.140 - ETA: 22s - loss: -3.1392 - acc: 0.142 - ETA: 22s - loss: -2.6500 - acc: 0.131 - ETA: 22s - loss: -2.7386 - acc: 0.127 - ETA: 22s - loss: -2.9499 - acc: 0.119 - ETA: 22s - loss: -2.8651 - acc: 0.113 - ETA: 22s - loss: -2.8052 - acc: 0.109 - ETA: 22s - loss: -2.6851 - acc: 0.110 - ETA: 22s - loss: -2.6107 - acc: 0.111 - ETA: 22s - loss: -2.4186 - acc: 0.117 - ETA: 22s - loss: -2.3342 - acc: 0.123 - ETA: 22s - loss: -2.2096 - acc: 0.123 - ETA: 21s - loss: -2.1308 - acc: 0.125 - ETA: 22s - loss: -2.1519 - acc: 0.125 - ETA: 21s - loss: -2.1169 - acc: 0.130 - ETA: 21s - loss: -2.1625 - acc: 0.137 - ETA: 21s - loss: -2.1964 - acc: 0.139 - ETA: 21s - loss: -1.9714 - acc: 0.146 - ETA: 21s - loss: -1.8766 - acc: 0.146 - ETA: 21s - loss: -1.7523 - acc: 0.151 - ETA: 21s - loss: -1.7996 - acc: 0.154 - ETA: 21s - loss: -1.7518 - acc: 0.158 - ETA: 21s - loss: -1.7723 - acc: 0.161 - ETA: 21s - loss: -1.7278 - acc: 0.168 - ETA: 21s - loss: -1.7039 - acc: 0.171 - ETA: 21s - loss: -1.6747 - acc: 0.177 - ETA: 21s - loss: -1.7609 - acc: 0.176 - ETA: 20s - loss: -1.7029 - acc: 0.175 - ETA: 20s - loss: -1.6715 - acc: 0.176 - ETA: 20s - loss: -1.5268 - acc: 0.177 - ETA: 20s - loss: -1.4615 - acc: 0.178 - ETA: 20s - loss: -1.4707 - acc: 0.179 - ETA: 20s - loss: -1.4738 - acc: 0.183 - ETA: 20s - loss: -1.4059 - acc: 0.183 - ETA: 20s - loss: -1.4160 - acc: 0.185 - ETA: 20s - loss: -1.4136 - acc: 0.186 - ETA: 20s - loss: -1.4146 - acc: 0.186 - ETA: 20s - loss: -1.3671 - acc: 0.186 - ETA: 20s - loss: -1.4094 - acc: 0.186 - ETA: 19s - loss: -1.3526 - acc: 0.186 - ETA: 19s - loss: -1.3857 - acc: 0.187 - ETA: 19s - loss: -1.3518 - acc: 0.190 - ETA: 19s - loss: -1.4067 - acc: 0.191 - ETA: 19s - loss: -1.3626 - acc: 0.192 - ETA: 19s - loss: -1.3612 - acc: 0.191 - ETA: 19s - loss: -1.3565 - acc: 0.194 - ETA: 19s - loss: -1.3488 - acc: 0.193 - ETA: 19s - loss: -1.3574 - acc: 0.194 - ETA: 19s - loss: -1.3986 - acc: 0.198 - ETA: 18s - loss: -1.4592 - acc: 0.198 - ETA: 18s - loss: -1.4492 - acc: 0.197 - ETA: 18s - loss: -1.4341 - acc: 0.197 - ETA: 18s - loss: -1.3745 - acc: 0.197 - ETA: 18s - loss: -1.3279 - acc: 0.201 - ETA: 18s - loss: -1.2974 - acc: 0.203 - ETA: 18s - loss: -1.3202 - acc: 0.204 - ETA: 18s - loss: -1.2118 - acc: 0.203 - ETA: 18s - loss: -1.2375 - acc: 0.204 - ETA: 18s - loss: -1.2633 - acc: 0.205 - ETA: 17s - loss: -1.3108 - acc: 0.205 - ETA: 17s - loss: -1.3333 - acc: 0.205 - ETA: 17s - loss: -1.3392 - acc: 0.205 - ETA: 17s - loss: -1.3276 - acc: 0.205 - ETA: 17s - loss: -1.3717 - acc: 0.208 - ETA: 17s - loss: -1.3574 - acc: 0.209 - ETA: 17s - loss: -1.3831 - acc: 0.208 - ETA: 17s - loss: -1.4214 - acc: 0.208 - ETA: 17s - loss: -1.4270 - acc: 0.207 - ETA: 17s - loss: -1.4474 - acc: 0.208 - ETA: 16s - loss: -1.4761 - acc: 0.207 - ETA: 16s - loss: -1.4747 - acc: 0.206 - ETA: 16s - loss: -1.4804 - acc: 0.207 - ETA: 16s - loss: -1.5155 - acc: 0.207 - ETA: 16s - loss: -1.5350 - acc: 0.206 - ETA: 16s - loss: -1.5238 - acc: 0.208 - ETA: 16s - loss: -1.5122 - acc: 0.209 - ETA: 16s - loss: -1.5175 - acc: 0.210 - ETA: 16s - loss: -1.4986 - acc: 0.211 - ETA: 16s - loss: -1.5324 - acc: 0.210 - ETA: 15s - loss: -1.5424 - acc: 0.210 - ETA: 15s - loss: -1.5601 - acc: 0.208 - ETA: 15s - loss: -1.5729 - acc: 0.208 - ETA: 15s - loss: -1.5854 - acc: 0.208 - ETA: 15s - loss: -1.6154 - acc: 0.209 - ETA: 15s - loss: -1.6224 - acc: 0.208 - ETA: 15s - loss: -1.6015 - acc: 0.208 - ETA: 15s - loss: -1.6088 - acc: 0.207 - ETA: 15s - loss: -1.5953 - acc: 0.207 - ETA: 15s - loss: -1.5907 - acc: 0.207 - ETA: 15s - loss: -1.5954 - acc: 0.207 - ETA: 14s - loss: -1.6082 - acc: 0.206 - ETA: 14s - loss: -1.5897 - acc: 0.206 - ETA: 14s - loss: -1.5861 - acc: 0.206 - ETA: 14s - loss: -1.5902 - acc: 0.207 - ETA: 14s - loss: -1.5831 - acc: 0.208 - ETA: 14s - loss: -1.5727 - acc: 0.209 - ETA: 14s - loss: -1.5566 - acc: 0.208 - ETA: 14s - loss: -1.5706 - acc: 0.208 - ETA: 14s - loss: -1.5883 - acc: 0.207 - ETA: 14s - loss: -1.5918 - acc: 0.206 - ETA: 13s - loss: -1.6281 - acc: 0.205 - ETA: 13s - loss: -1.6415 - acc: 0.206 - ETA: 13s - loss: -1.6465 - acc: 0.206 - ETA: 13s - loss: -1.6611 - acc: 0.208 - ETA: 13s - loss: -1.6849 - acc: 0.207 - ETA: 13s - loss: -1.6931 - acc: 0.207 - ETA: 13s - loss: -1.6998 - acc: 0.206 - ETA: 13s - loss: -1.6925 - acc: 0.206 - ETA: 13s - loss: -1.7259 - acc: 0.206 - ETA: 13s - loss: -1.7104 - acc: 0.206 - ETA: 13s - loss: -1.7002 - acc: 0.206 - ETA: 12s - loss: -1.6909 - acc: 0.206 - ETA: 12s - loss: -1.6975 - acc: 0.206 - ETA: 12s - loss: -1.6723 - acc: 0.206 - ETA: 12s - loss: -1.6814 - acc: 0.206 - ETA: 12s - loss: -1.6950 - acc: 0.207 - ETA: 12s - loss: -1.7118 - acc: 0.206 - ETA: 12s - loss: -1.7120 - acc: 0.206 - ETA: 12s - loss: -1.7003 - acc: 0.206 - ETA: 12s - loss: -1.6932 - acc: 0.206 - ETA: 12s - loss: -1.6648 - acc: 0.206 - ETA: 11s - loss: -1.6429 - acc: 0.205 - ETA: 11s - loss: -1.6320 - acc: 0.206 - ETA: 11s - loss: -1.6669 - acc: 0.205 - ETA: 11s - loss: -1.6618 - acc: 0.206 - ETA: 11s - loss: -1.6421 - acc: 0.206 - ETA: 11s - loss: -1.6496 - acc: 0.205 - ETA: 11s - loss: -1.6575 - acc: 0.207 - ETA: 11s - loss: -1.6737 - acc: 0.206 - ETA: 11s - loss: -1.6785 - acc: 0.207 - ETA: 11s - loss: -1.6824 - acc: 0.207 - ETA: 11s - loss: -1.6729 - acc: 0.207 - ETA: 10s - loss: -1.6612 - acc: 0.207 - ETA: 10s - loss: -1.6553 - acc: 0.207 - ETA: 10s - loss: -1.6558 - acc: 0.208 - ETA: 10s - loss: -1.6463 - acc: 0.208 - ETA: 10s - loss: -1.6541 - acc: 0.207 - ETA: 10s - loss: -1.6600 - acc: 0.207 - ETA: 10s - loss: -1.6814 - acc: 0.208 - ETA: 10s - loss: -1.6969 - acc: 0.209 - ETA: 10s - loss: -1.6768 - acc: 0.209 - ETA: 10s - loss: -1.6701 - acc: 0.210 - ETA: 9s - loss: -1.6599 - acc: 0.210 - ETA: 9s - loss: -1.6623 - acc: 0.21 - ETA: 9s - loss: -1.6572 - acc: 0.21 - ETA: 9s - loss: -1.6747 - acc: 0.21 - ETA: 9s - loss: -1.6855 - acc: 0.21 - ETA: 9s - loss: -1.6843 - acc: 0.21 - ETA: 9s - loss: -1.6888 - acc: 0.21 - ETA: 9s - loss: -1.6972 - acc: 0.21 - ETA: 9s - loss: -1.6981 - acc: 0.21 - ETA: 9s - loss: -1.7023 - acc: 0.21 - ETA: 8s - loss: -1.7054 - acc: 0.21 - ETA: 8s - loss: -1.7071 - acc: 0.22 - ETA: 8s - loss: -1.7048 - acc: 0.22 - ETA: 8s - loss: -1.6969 - acc: 0.22 - ETA: 8s - loss: -1.6989 - acc: 0.22 - ETA: 8s - loss: -1.6967 - acc: 0.22 - ETA: 8s - loss: -1.7241 - acc: 0.22 - ETA: 8s - loss: -1.7304 - acc: 0.22 - ETA: 8s - loss: -1.7278 - acc: 0.22 - ETA: 8s - loss: -1.7280 - acc: 0.22 - ETA: 8s - loss: -1.6930 - acc: 0.21 - ETA: 7s - loss: -1.7027 - acc: 0.21 - ETA: 7s - loss: -1.7031 - acc: 0.21 - ETA: 7s - loss: -1.7045 - acc: 0.21 - ETA: 7s - loss: -1.7018 - acc: 0.21 - ETA: 7s - loss: -1.7075 - acc: 0.22 - ETA: 7s - loss: -1.7070 - acc: 0.22 - ETA: 7s - loss: -1.7231 - acc: 0.22 - ETA: 7s - loss: -1.7430 - acc: 0.22 - ETA: 7s - loss: -1.7548 - acc: 0.22 - ETA: 7s - loss: -1.7612 - acc: 0.22 - ETA: 6s - loss: -1.7644 - acc: 0.22 - ETA: 6s - loss: -1.7531 - acc: 0.22 - ETA: 6s - loss: -1.7511 - acc: 0.22 - ETA: 6s - loss: -1.7438 - acc: 0.22 - ETA: 6s - loss: -1.7365 - acc: 0.22 - ETA: 6s - loss: -1.7430 - acc: 0.22 - ETA: 6s - loss: -1.7484 - acc: 0.22 - ETA: 6s - loss: -1.7568 - acc: 0.22 - ETA: 6s - loss: -1.7475 - acc: 0.22 - ETA: 6s - loss: -1.7472 - acc: 0.22 - ETA: 6s - loss: -1.7501 - acc: 0.22 - ETA: 5s - loss: -1.7590 - acc: 0.22 - ETA: 5s - loss: -1.7637 - acc: 0.22 - ETA: 5s - loss: -1.7735 - acc: 0.23 - ETA: 5s - loss: -1.7864 - acc: 0.23 - ETA: 5s - loss: -1.7859 - acc: 0.23 - ETA: 5s - loss: -1.7830 - acc: 0.23 - ETA: 5s - loss: -1.7631 - acc: 0.23 - ETA: 5s - loss: -1.7699 - acc: 0.23 - ETA: 5s - loss: -1.7826 - acc: 0.23 - ETA: 5s - loss: -1.7999 - acc: 0.23 - ETA: 4s - loss: -1.7832 - acc: 0.23 - ETA: 4s - loss: -1.7759 - acc: 0.23 - ETA: 4s - loss: -1.7844 - acc: 0.23 - ETA: 4s - loss: -1.7894 - acc: 0.23 - ETA: 4s - loss: -1.7928 - acc: 0.23 - ETA: 4s - loss: -1.8034 - acc: 0.2338"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8126/8126 [==============================] - ETA: 4s - loss: -1.8082 - acc: 0.23 - ETA: 4s - loss: -1.7973 - acc: 0.23 - ETA: 4s - loss: -1.7806 - acc: 0.23 - ETA: 4s - loss: -1.7882 - acc: 0.23 - ETA: 4s - loss: -1.8029 - acc: 0.23 - ETA: 3s - loss: -1.7937 - acc: 0.23 - ETA: 3s - loss: -1.7832 - acc: 0.23 - ETA: 3s - loss: -1.7904 - acc: 0.23 - ETA: 3s - loss: -1.7931 - acc: 0.23 - ETA: 3s - loss: -1.8020 - acc: 0.23 - ETA: 3s - loss: -1.7890 - acc: 0.23 - ETA: 3s - loss: -1.7762 - acc: 0.23 - ETA: 3s - loss: -1.7733 - acc: 0.23 - ETA: 3s - loss: -1.7697 - acc: 0.23 - ETA: 3s - loss: -1.7710 - acc: 0.23 - ETA: 2s - loss: -1.7747 - acc: 0.23 - ETA: 2s - loss: -1.7682 - acc: 0.23 - ETA: 2s - loss: -1.7696 - acc: 0.23 - ETA: 2s - loss: -1.7682 - acc: 0.23 - ETA: 2s - loss: -1.7696 - acc: 0.23 - ETA: 2s - loss: -1.7653 - acc: 0.23 - ETA: 2s - loss: -1.7656 - acc: 0.23 - ETA: 2s - loss: -1.7546 - acc: 0.23 - ETA: 2s - loss: -1.7502 - acc: 0.23 - ETA: 2s - loss: -1.7602 - acc: 0.23 - ETA: 2s - loss: -1.7474 - acc: 0.23 - ETA: 1s - loss: -1.7371 - acc: 0.23 - ETA: 1s - loss: -1.7366 - acc: 0.23 - ETA: 1s - loss: -1.7304 - acc: 0.23 - ETA: 1s - loss: -1.7382 - acc: 0.23 - ETA: 1s - loss: -1.7337 - acc: 0.23 - ETA: 1s - loss: -1.7403 - acc: 0.23 - ETA: 1s - loss: -1.7353 - acc: 0.23 - ETA: 1s - loss: -1.7263 - acc: 0.23 - ETA: 1s - loss: -1.7221 - acc: 0.23 - ETA: 1s - loss: -1.7190 - acc: 0.23 - ETA: 0s - loss: -1.7062 - acc: 0.23 - ETA: 0s - loss: -1.7045 - acc: 0.23 - ETA: 0s - loss: -1.7097 - acc: 0.23 - ETA: 0s - loss: -1.7040 - acc: 0.23 - ETA: 0s - loss: -1.6962 - acc: 0.23 - ETA: 0s - loss: -1.7038 - acc: 0.23 - ETA: 0s - loss: -1.7073 - acc: 0.23 - ETA: 0s - loss: -1.7192 - acc: 0.23 - ETA: 0s - loss: -1.7196 - acc: 0.23 - ETA: 0s - loss: -1.7136 - acc: 0.23 - 25s 3ms/step - loss: -1.7258 - acc: 0.2325 - val_loss: -0.6079 - val_acc: 0.2230\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6624/8126 [=======================>......] - ETA: 22s - loss: -3.1424 - acc: 0.343 - ETA: 22s - loss: -3.0089 - acc: 0.265 - ETA: 24s - loss: -2.9487 - acc: 0.250 - ETA: 23s - loss: -3.4715 - acc: 0.242 - ETA: 23s - loss: -3.0975 - acc: 0.225 - ETA: 23s - loss: -2.9808 - acc: 0.234 - ETA: 23s - loss: -3.2246 - acc: 0.227 - ETA: 23s - loss: -3.2348 - acc: 0.230 - ETA: 23s - loss: -3.3529 - acc: 0.225 - ETA: 23s - loss: -3.5025 - acc: 0.234 - ETA: 23s - loss: -3.2514 - acc: 0.241 - ETA: 23s - loss: -3.3627 - acc: 0.247 - ETA: 22s - loss: -3.4332 - acc: 0.245 - ETA: 22s - loss: -3.2604 - acc: 0.247 - ETA: 22s - loss: -3.2128 - acc: 0.241 - ETA: 22s - loss: -3.2746 - acc: 0.236 - ETA: 22s - loss: -3.2312 - acc: 0.239 - ETA: 22s - loss: -3.3456 - acc: 0.232 - ETA: 22s - loss: -3.4981 - acc: 0.231 - ETA: 22s - loss: -3.5944 - acc: 0.240 - ETA: 22s - loss: -3.4474 - acc: 0.248 - ETA: 22s - loss: -3.4953 - acc: 0.247 - ETA: 21s - loss: -3.4706 - acc: 0.244 - ETA: 21s - loss: -3.4712 - acc: 0.244 - ETA: 21s - loss: -3.5264 - acc: 0.245 - ETA: 21s - loss: -3.5326 - acc: 0.246 - ETA: 21s - loss: -3.4160 - acc: 0.253 - ETA: 21s - loss: -3.4335 - acc: 0.252 - ETA: 21s - loss: -3.4732 - acc: 0.252 - ETA: 21s - loss: -3.4531 - acc: 0.256 - ETA: 21s - loss: -3.4210 - acc: 0.259 - ETA: 21s - loss: -3.4889 - acc: 0.262 - ETA: 21s - loss: -3.3769 - acc: 0.269 - ETA: 20s - loss: -3.4328 - acc: 0.270 - ETA: 20s - loss: -3.4205 - acc: 0.277 - ETA: 20s - loss: -3.4392 - acc: 0.283 - ETA: 20s - loss: -3.4443 - acc: 0.289 - ETA: 20s - loss: -3.4803 - acc: 0.292 - ETA: 20s - loss: -3.5495 - acc: 0.299 - ETA: 20s - loss: -3.4911 - acc: 0.302 - ETA: 20s - loss: -3.5249 - acc: 0.301 - ETA: 20s - loss: -3.5046 - acc: 0.305 - ETA: 20s - loss: -3.4557 - acc: 0.306 - ETA: 20s - loss: -3.4620 - acc: 0.306 - ETA: 20s - loss: -3.4386 - acc: 0.304 - ETA: 19s - loss: -3.3765 - acc: 0.307 - ETA: 19s - loss: -3.3747 - acc: 0.308 - ETA: 19s - loss: -3.3933 - acc: 0.308 - ETA: 19s - loss: -3.3884 - acc: 0.308 - ETA: 19s - loss: -3.3753 - acc: 0.310 - ETA: 19s - loss: -3.3296 - acc: 0.313 - ETA: 19s - loss: -3.2858 - acc: 0.312 - ETA: 19s - loss: -3.2812 - acc: 0.313 - ETA: 19s - loss: -3.2988 - acc: 0.314 - ETA: 19s - loss: -3.3309 - acc: 0.315 - ETA: 18s - loss: -3.3535 - acc: 0.312 - ETA: 18s - loss: -3.3519 - acc: 0.312 - ETA: 18s - loss: -3.3721 - acc: 0.314 - ETA: 18s - loss: -3.3969 - acc: 0.314 - ETA: 18s - loss: -3.3479 - acc: 0.317 - ETA: 18s - loss: -3.4050 - acc: 0.314 - ETA: 18s - loss: -3.4141 - acc: 0.313 - ETA: 18s - loss: -3.3663 - acc: 0.315 - ETA: 18s - loss: -3.3432 - acc: 0.314 - ETA: 18s - loss: -3.3067 - acc: 0.315 - ETA: 17s - loss: -3.2865 - acc: 0.315 - ETA: 17s - loss: -3.3041 - acc: 0.315 - ETA: 17s - loss: -3.2719 - acc: 0.317 - ETA: 17s - loss: -3.2811 - acc: 0.315 - ETA: 17s - loss: -3.2608 - acc: 0.315 - ETA: 17s - loss: -3.2275 - acc: 0.319 - ETA: 17s - loss: -3.2380 - acc: 0.320 - ETA: 17s - loss: -3.2301 - acc: 0.322 - ETA: 17s - loss: -3.2455 - acc: 0.322 - ETA: 17s - loss: -3.2168 - acc: 0.322 - ETA: 17s - loss: -3.2231 - acc: 0.322 - ETA: 16s - loss: -3.2153 - acc: 0.321 - ETA: 16s - loss: -3.2131 - acc: 0.322 - ETA: 16s - loss: -3.2105 - acc: 0.322 - ETA: 16s - loss: -3.2206 - acc: 0.323 - ETA: 16s - loss: -3.1705 - acc: 0.326 - ETA: 16s - loss: -3.1362 - acc: 0.327 - ETA: 16s - loss: -3.1014 - acc: 0.329 - ETA: 16s - loss: -3.0573 - acc: 0.330 - ETA: 16s - loss: -3.0531 - acc: 0.330 - ETA: 16s - loss: -3.0580 - acc: 0.330 - ETA: 15s - loss: -3.0523 - acc: 0.329 - ETA: 15s - loss: -3.0891 - acc: 0.330 - ETA: 15s - loss: -3.0611 - acc: 0.331 - ETA: 15s - loss: -3.0671 - acc: 0.333 - ETA: 15s - loss: -3.0550 - acc: 0.332 - ETA: 15s - loss: -3.0478 - acc: 0.333 - ETA: 15s - loss: -3.0385 - acc: 0.335 - ETA: 15s - loss: -3.0382 - acc: 0.336 - ETA: 15s - loss: -3.0208 - acc: 0.336 - ETA: 15s - loss: -2.9917 - acc: 0.338 - ETA: 15s - loss: -3.0278 - acc: 0.337 - ETA: 14s - loss: -3.0323 - acc: 0.338 - ETA: 14s - loss: -3.0612 - acc: 0.338 - ETA: 14s - loss: -3.0777 - acc: 0.339 - ETA: 14s - loss: -3.0744 - acc: 0.338 - ETA: 14s - loss: -3.0609 - acc: 0.339 - ETA: 14s - loss: -3.0660 - acc: 0.338 - ETA: 14s - loss: -3.0759 - acc: 0.338 - ETA: 14s - loss: -3.0710 - acc: 0.339 - ETA: 14s - loss: -3.0593 - acc: 0.341 - ETA: 14s - loss: -3.0655 - acc: 0.342 - ETA: 13s - loss: -3.0863 - acc: 0.341 - ETA: 13s - loss: -3.0986 - acc: 0.340 - ETA: 13s - loss: -3.1253 - acc: 0.340 - ETA: 13s - loss: -3.1240 - acc: 0.338 - ETA: 13s - loss: -3.1466 - acc: 0.338 - ETA: 13s - loss: -3.1704 - acc: 0.338 - ETA: 13s - loss: -3.1507 - acc: 0.338 - ETA: 13s - loss: -3.1560 - acc: 0.339 - ETA: 13s - loss: -3.1441 - acc: 0.339 - ETA: 13s - loss: -3.1494 - acc: 0.338 - ETA: 13s - loss: -3.1601 - acc: 0.337 - ETA: 12s - loss: -3.1600 - acc: 0.337 - ETA: 12s - loss: -3.1653 - acc: 0.336 - ETA: 12s - loss: -3.1634 - acc: 0.337 - ETA: 12s - loss: -3.1521 - acc: 0.337 - ETA: 12s - loss: -3.1707 - acc: 0.337 - ETA: 12s - loss: -3.1486 - acc: 0.337 - ETA: 12s - loss: -3.1438 - acc: 0.338 - ETA: 12s - loss: -3.1381 - acc: 0.338 - ETA: 12s - loss: -3.1414 - acc: 0.338 - ETA: 12s - loss: -3.1529 - acc: 0.338 - ETA: 12s - loss: -3.1455 - acc: 0.338 - ETA: 11s - loss: -3.1567 - acc: 0.338 - ETA: 11s - loss: -3.1543 - acc: 0.340 - ETA: 11s - loss: -3.1402 - acc: 0.340 - ETA: 11s - loss: -3.1430 - acc: 0.340 - ETA: 11s - loss: -3.1595 - acc: 0.340 - ETA: 11s - loss: -3.1586 - acc: 0.340 - ETA: 11s - loss: -3.1696 - acc: 0.339 - ETA: 11s - loss: -3.1766 - acc: 0.339 - ETA: 11s - loss: -3.1863 - acc: 0.340 - ETA: 11s - loss: -3.1957 - acc: 0.340 - ETA: 11s - loss: -3.1804 - acc: 0.340 - ETA: 10s - loss: -3.1701 - acc: 0.341 - ETA: 10s - loss: -3.1787 - acc: 0.341 - ETA: 10s - loss: -3.2063 - acc: 0.340 - ETA: 10s - loss: -3.2123 - acc: 0.340 - ETA: 10s - loss: -3.2216 - acc: 0.340 - ETA: 10s - loss: -3.2025 - acc: 0.341 - ETA: 10s - loss: -3.1978 - acc: 0.341 - ETA: 10s - loss: -3.1961 - acc: 0.342 - ETA: 10s - loss: -3.2080 - acc: 0.341 - ETA: 10s - loss: -3.1917 - acc: 0.341 - ETA: 9s - loss: -3.2134 - acc: 0.341 - ETA: 9s - loss: -3.2174 - acc: 0.34 - ETA: 9s - loss: -3.2136 - acc: 0.34 - ETA: 9s - loss: -3.2275 - acc: 0.34 - ETA: 9s - loss: -3.2140 - acc: 0.34 - ETA: 9s - loss: -3.2393 - acc: 0.34 - ETA: 9s - loss: -3.2601 - acc: 0.34 - ETA: 9s - loss: -3.2570 - acc: 0.34 - ETA: 9s - loss: -3.2594 - acc: 0.34 - ETA: 9s - loss: -3.2580 - acc: 0.34 - ETA: 8s - loss: -3.2624 - acc: 0.34 - ETA: 8s - loss: -3.2577 - acc: 0.34 - ETA: 8s - loss: -3.2388 - acc: 0.34 - ETA: 8s - loss: -3.2519 - acc: 0.34 - ETA: 8s - loss: -3.2666 - acc: 0.34 - ETA: 8s - loss: -3.2666 - acc: 0.34 - ETA: 8s - loss: -3.2632 - acc: 0.34 - ETA: 8s - loss: -3.2692 - acc: 0.34 - ETA: 8s - loss: -3.2756 - acc: 0.34 - ETA: 8s - loss: -3.2593 - acc: 0.34 - ETA: 8s - loss: -3.2433 - acc: 0.34 - ETA: 7s - loss: -3.2383 - acc: 0.34 - ETA: 7s - loss: -3.2357 - acc: 0.34 - ETA: 7s - loss: -3.2268 - acc: 0.34 - ETA: 7s - loss: -3.2354 - acc: 0.34 - ETA: 7s - loss: -3.2425 - acc: 0.34 - ETA: 7s - loss: -3.2403 - acc: 0.34 - ETA: 7s - loss: -3.2555 - acc: 0.34 - ETA: 7s - loss: -3.2657 - acc: 0.34 - ETA: 7s - loss: -3.2543 - acc: 0.34 - ETA: 7s - loss: -3.2539 - acc: 0.34 - ETA: 6s - loss: -3.2498 - acc: 0.34 - ETA: 6s - loss: -3.2580 - acc: 0.34 - ETA: 6s - loss: -3.2660 - acc: 0.34 - ETA: 6s - loss: -3.2551 - acc: 0.34 - ETA: 6s - loss: -3.2658 - acc: 0.34 - ETA: 6s - loss: -3.2679 - acc: 0.34 - ETA: 6s - loss: -3.2731 - acc: 0.33 - ETA: 6s - loss: -3.2861 - acc: 0.34 - ETA: 6s - loss: -3.2792 - acc: 0.34 - ETA: 6s - loss: -3.2754 - acc: 0.34 - ETA: 5s - loss: -3.2605 - acc: 0.34 - ETA: 5s - loss: -3.2588 - acc: 0.34 - ETA: 5s - loss: -3.2473 - acc: 0.34 - ETA: 5s - loss: -3.2474 - acc: 0.33 - ETA: 5s - loss: -3.2435 - acc: 0.33 - ETA: 5s - loss: -3.2472 - acc: 0.33 - ETA: 5s - loss: -3.2491 - acc: 0.33 - ETA: 5s - loss: -3.2521 - acc: 0.33 - ETA: 5s - loss: -3.2641 - acc: 0.33 - ETA: 5s - loss: -3.2749 - acc: 0.33 - ETA: 5s - loss: -3.2694 - acc: 0.33 - ETA: 4s - loss: -3.2790 - acc: 0.33 - ETA: 4s - loss: -3.2721 - acc: 0.33 - ETA: 4s - loss: -3.2736 - acc: 0.33 - ETA: 4s - loss: -3.2735 - acc: 0.33 - ETA: 4s - loss: -3.2799 - acc: 0.3370"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8126/8126 [==============================] - ETA: 4s - loss: -3.2713 - acc: 0.33 - ETA: 4s - loss: -3.2725 - acc: 0.33 - ETA: 4s - loss: -3.2637 - acc: 0.33 - ETA: 4s - loss: -3.2587 - acc: 0.33 - ETA: 4s - loss: -3.2518 - acc: 0.33 - ETA: 3s - loss: -3.2607 - acc: 0.33 - ETA: 3s - loss: -3.2640 - acc: 0.33 - ETA: 3s - loss: -3.2415 - acc: 0.33 - ETA: 3s - loss: -3.2376 - acc: 0.33 - ETA: 3s - loss: -3.2356 - acc: 0.33 - ETA: 3s - loss: -3.2284 - acc: 0.33 - ETA: 3s - loss: -3.2300 - acc: 0.33 - ETA: 3s - loss: -3.2468 - acc: 0.33 - ETA: 3s - loss: -3.2504 - acc: 0.33 - ETA: 3s - loss: -3.2544 - acc: 0.33 - ETA: 2s - loss: -3.2511 - acc: 0.33 - ETA: 2s - loss: -3.2362 - acc: 0.33 - ETA: 2s - loss: -3.2356 - acc: 0.33 - ETA: 2s - loss: -3.2327 - acc: 0.33 - ETA: 2s - loss: -3.2274 - acc: 0.33 - ETA: 2s - loss: -3.2292 - acc: 0.33 - ETA: 2s - loss: -3.2383 - acc: 0.33 - ETA: 2s - loss: -3.2400 - acc: 0.33 - ETA: 2s - loss: -3.2406 - acc: 0.33 - ETA: 2s - loss: -3.2350 - acc: 0.33 - ETA: 2s - loss: -3.2431 - acc: 0.33 - ETA: 1s - loss: -3.2486 - acc: 0.33 - ETA: 1s - loss: -3.2509 - acc: 0.33 - ETA: 1s - loss: -3.2538 - acc: 0.33 - ETA: 1s - loss: -3.2543 - acc: 0.33 - ETA: 1s - loss: -3.2496 - acc: 0.33 - ETA: 1s - loss: -3.2548 - acc: 0.33 - ETA: 1s - loss: -3.2498 - acc: 0.33 - ETA: 1s - loss: -3.2458 - acc: 0.33 - ETA: 1s - loss: -3.2525 - acc: 0.33 - ETA: 1s - loss: -3.2600 - acc: 0.33 - ETA: 0s - loss: -3.2576 - acc: 0.33 - ETA: 0s - loss: -3.2418 - acc: 0.33 - ETA: 0s - loss: -3.2510 - acc: 0.33 - ETA: 0s - loss: -3.2527 - acc: 0.33 - ETA: 0s - loss: -3.2443 - acc: 0.33 - ETA: 0s - loss: -3.2325 - acc: 0.33 - ETA: 0s - loss: -3.2387 - acc: 0.33 - ETA: 0s - loss: -3.2365 - acc: 0.33 - ETA: 0s - loss: -3.2331 - acc: 0.33 - ETA: 0s - loss: -3.2369 - acc: 0.33 - 25s 3ms/step - loss: -3.2349 - acc: 0.3337 - val_loss: -0.3178 - val_acc: 0.3060\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, tr_file['b_label'][(tr_file['b_label']!=0)],\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(X_dev, va_file['b_label'][(va_file['b_label']!=0)]),\n",
    "                    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
